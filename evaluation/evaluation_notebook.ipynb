{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# News Recommendation System Evaluation\n",
        "\n",
        "This notebook evaluates the performance of our news recommendation system across multiple dimensions:\n",
        "- Search Quality (Retrieval)\n",
        "- Recommendation Accuracy\n",
        "- Personalization Effectiveness\n",
        "- System Performance\n",
        "\n",
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('src')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Core system imports\n",
        "from storage import ArticleDB\n",
        "from embeddings import EmbeddingSystem\n",
        "from retrieval import MultiRAGRetriever\n",
        "from scoring import ScoringEngine\n",
        "from reranker import RerankingEngine\n",
        "from recommendation_learner import AIRecommender\n",
        "from data_models import Article, UserProfile, SearchQuery\n",
        "from config import RAGConfig, EmbeddingModelConfig\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. System Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize core components\n",
        "print(\" Initializing system components...\")\n",
        "\n",
        "# Database\n",
        "db = ArticleDB()\n",
        "print(f\" Database: {len(db.get_all_articles())} articles\")\n",
        "\n",
        "# Embeddings\n",
        "embeddings = EmbeddingSystem()\n",
        "print(f\" Embeddings: {embeddings.model_name} loaded\")\n",
        "\n",
        "# Retrieval system\n",
        "retriever = MultiRAGRetriever(db, embeddings)\n",
        "print(\" MultiRAG Retriever ready\")\n",
        "\n",
        "# Scoring engine\n",
        "config = RAGConfig()\n",
        "scoring_engine = ScoringEngine(config)\n",
        "print(\" Scoring engine ready\")\n",
        "\n",
        "# Recommendation system\n",
        "recommender = AIRecommender(db, embeddings)\n",
        "print(\" Recommendation system ready\")\n",
        "\n",
        "print(\"\\n System initialization complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Experiment 1: Search Quality Evaluation\n",
        "\n",
        "**Objective**: Evaluate retrieval performance using precision, recall, and relevance scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_search_quality(retriever: MultiRAGRetriever, test_queries: List[Dict]) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate search quality using test queries with known relevant articles.\n",
        "    \n",
        "    Args:\n",
        "        retriever: MultiRAGRetriever instance\n",
        "        test_queries: List of test queries with expected results\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with evaluation metrics\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        'precision_at_5': [],\n",
        "        'precision_at_10': [],\n",
        "        'recall_at_10': [],\n",
        "        'mrr': [],  # Mean Reciprocal Rank\n",
        "        'query_times': []\n",
        "    }\n",
        "    \n",
        "    for query_data in test_queries:\n",
        "        query = query_data['query']\n",
        "        expected_articles = set(query_data['expected_articles'])\n",
        "        \n",
        "        # Measure query time\n",
        "        start_time = time.time()\n",
        "        search_results = retriever.search(query, limit=10)\n",
        "        query_time = time.time() - start_time\n",
        "        \n",
        "        # Extract article IDs from results\n",
        "        retrieved_articles = set([result.article_id for result in search_results])\n",
        "        \n",
        "        # Calculate metrics\n",
        "        relevant_retrieved = retrieved_articles.intersection(expected_articles)\n",
        "        \n",
        "        # Precision@K\n",
        "        precision_5 = len(relevant_retrieved.intersection(set([r.article_id for r in search_results[:5]]))) / 5\n",
        "        precision_10 = len(relevant_retrieved) / 10\n",
        "        \n",
        "        # Recall@10\n",
        "        recall_10 = len(relevant_retrieved) / len(expected_articles) if expected_articles else 0\n",
        "        \n",
        "        # MRR (Mean Reciprocal Rank)\n",
        "        mrr = 0\n",
        "        for i, result in enumerate(search_results):\n",
        "            if result.article_id in expected_articles:\n",
        "                mrr = 1.0 / (i + 1)\n",
        "                break\n",
        "        \n",
        "        results['precision_at_5'].append(precision_5)\n",
        "        results['precision_at_10'].append(precision_10)\n",
        "        results['recall_at_10'].append(recall_10)\n",
        "        results['mrr'].append(mrr)\n",
        "        results['query_times'].append(query_time)\n",
        "    \n",
        "    # Calculate averages\n",
        "    return {\n",
        "        'avg_precision_at_5': np.mean(results['precision_at_5']),\n",
        "        'avg_precision_at_10': np.mean(results['precision_at_10']),\n",
        "        'avg_recall_at_10': np.mean(results['recall_at_10']),\n",
        "        'avg_mrr': np.mean(results['mrr']),\n",
        "        'avg_query_time': np.mean(results['query_times']),\n",
        "        'total_queries': len(test_queries)\n",
        "    }\n",
        "\n",
        "# Load SPICED dataset for proper evaluation\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def load_spiced_dataset():\n",
        "    \"\"\"Load SPICED dataset for evaluation.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv('evaluation/spiced.csv')\n",
        "        print(f\"SPICED dataset loaded: {len(df)} pairs\")\n",
        "        print(f\"Topics: {df['Type'].unique()}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load SPICED dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load SPICED dataset\n",
        "spiced_data = load_spiced_dataset()\n",
        "\n",
        "if spiced_data is not None:\n",
        "    print(f\"\\nDataset Statistics:\")\n",
        "    print(f\"Total pairs: {len(spiced_data)}\")\n",
        "    print(f\"Topics: {', '.join(spiced_data['Type'].unique())}\")\n",
        "    print(f\"Topic distribution:\")\n",
        "    print(spiced_data['Type'].value_counts())\n",
        "    \n",
        "    print(f\"\\nSample SPICED pairs:\")\n",
        "    for i, (idx, row) in enumerate(spiced_data.head(3).iterrows()):\n",
        "        print(f\"\\nPair {i+1} ({row['Type']}):\")\n",
        "        print(f\"  Text 1: {row['text_1'][:100]}...\")\n",
        "        print(f\"  Text 2: {row['text_2'][:100]}...\")\n",
        "        print(f\"  URL 1: {row['URL_1']}\")\n",
        "        print(f\"  URL 2: {row['URL_2']}\")\n",
        "else:\n",
        "    print(\"SPICED dataset not available. Using fallback evaluation.\")\n",
        "\n",
        "print(\"Running search quality evaluation...\")\n",
        "search_metrics = evaluate_search_quality(retriever, test_queries)\n",
        "\n",
        "print(\"\\n Search Quality Results:\")\n",
        "for metric, value in search_metrics.items():\n",
        "    if metric != 'total_queries':\n",
        "        print(f\"  {metric}: {value:.3f}\")\n",
        "    else:\n",
        "        print(f\"  {metric}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run comprehensive evaluation pipeline\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "def run_evaluation_pipeline():\n",
        "    \"\"\"Run the comprehensive evaluation pipeline.\"\"\"\n",
        "    try:\n",
        "        print(\"Running comprehensive evaluation pipeline...\")\n",
        "        result = subprocess.run(['python', 'evaluation/evaluation_pipeline.py'], \n",
        "                              capture_output=True, text=True, check=True)\n",
        "        print(\"Pipeline output:\")\n",
        "        print(result.stdout)\n",
        "        \n",
        "        # Load results\n",
        "        with open('evaluation/evaluation_results.json', 'r') as f:\n",
        "            results = json.load(f)\n",
        "        \n",
        "        return results\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Pipeline failed: {e}\")\n",
        "        print(f\"Error output: {e.stderr}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error running pipeline: {e}\")\n",
        "        return None\n",
        "\n",
        "# Run evaluation\n",
        "evaluation_results = run_evaluation_pipeline()\n",
        "\n",
        "if evaluation_results:\n",
        "    print(\"\\nEvaluation Results Summary:\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    metadata = evaluation_results.get('metadata', {})\n",
        "    print(f\"Evaluation Date: {metadata.get('evaluation_date', 'N/A')}\")\n",
        "    print(f\"Total Queries: {metadata.get('total_queries', 0)}\")\n",
        "    print(f\"SPICED Pairs: {metadata.get('spiced_pairs', 0)}\")\n",
        "    print(f\"Topics: {', '.join(metadata.get('topics', []))}\")\n",
        "    \n",
        "    # Overall performance\n",
        "    overall = evaluation_results.get('overall_performance', {})\n",
        "    print(f\"\\nOverall Performance:\")\n",
        "    print(f\"  Similarity Score: {overall.get('similarity_score', 0.0):.3f}\")\n",
        "    print(f\"  Topic Relevance: {overall.get('topic_relevance', 0.0):.3f}\")\n",
        "    print(f\"  Diversity Score: {overall.get('diversity_score', 0.0):.3f}\")\n",
        "    print(f\"  Intratopic Score: {overall.get('intratopic_score', 0.0):.3f}\")\n",
        "    print(f\"  Intertopic Score: {overall.get('intertopic_score', 0.0):.3f}\")\n",
        "    print(f\"  Hard Examples Score: {overall.get('hard_examples_score', 0.0):.3f}\")\n",
        "    print(f\"  Overall Score: {overall.get('overall_score', 0.0):.3f}\")\n",
        "    \n",
        "    # Difficulty-based performance\n",
        "    print(f\"\\nDifficulty-Based Performance:\")\n",
        "    if 'intratopic_performance' in evaluation_results:\n",
        "        intratopic = evaluation_results['intratopic_performance']\n",
        "        print(f\"  Intratopic (Same Topic): {intratopic.get('intratopic_score', 0.0):.3f} ({intratopic.get('total_pairs', 0)} pairs)\")\n",
        "    \n",
        "    if 'intertopic_performance' in evaluation_results:\n",
        "        intertopic = evaluation_results['intertopic_performance']\n",
        "        print(f\"  Intertopic (Cross Topic): {intertopic.get('intertopic_score', 0.0):.3f} ({intertopic.get('total_pairs', 0)} pairs)\")\n",
        "    \n",
        "    if 'hard_examples_performance' in evaluation_results:\n",
        "        hard_examples = evaluation_results['hard_examples_performance']\n",
        "        if hard_examples.get('total_pairs', 0) > 0:\n",
        "            print(f\"  Hard Examples: {hard_examples.get('hard_examples_score', 0.0):.3f} ({hard_examples.get('total_pairs', 0)} pairs)\")\n",
        "        else:\n",
        "            print(f\"  Hard Examples: No hard examples available\")\n",
        "    \n",
        "    # Topic-specific performance\n",
        "    if 'topic_retrieval' in evaluation_results and 'topic_metrics' in evaluation_results['topic_retrieval']:\n",
        "        print(f\"\\nTopic Performance:\")\n",
        "        for topic, metrics in evaluation_results['topic_retrieval']['topic_metrics'].items():\n",
        "            print(f\"  {topic}: {metrics['mean_relevance']:.3f} ({metrics['query_count']} queries)\")\n",
        "    \n",
        "    # Baseline comparison\n",
        "    if 'baseline_comparison' in evaluation_results:\n",
        "        print(f\"\\nBaseline Comparison:\")\n",
        "        baselines = evaluation_results['baseline_comparison']\n",
        "        for baseline_name, baseline_data in baselines.items():\n",
        "            method = baseline_data.get('method', baseline_name)\n",
        "            score = baseline_data.get('avg_score', 0.0)\n",
        "            queries = baseline_data.get('total_queries', 0)\n",
        "            print(f\"  {method}: {score:.3f} ({queries} queries)\")\n",
        "    \n",
        "    # Dataset statistics\n",
        "    metadata = evaluation_results.get('metadata', {})\n",
        "    print(f\"\\nDataset Statistics:\")\n",
        "    print(f\"  Train Pairs: {metadata.get('train_pairs', 0)}\")\n",
        "    print(f\"  Test Pairs: {metadata.get('test_pairs', 0)}\")\n",
        "    print(f\"  Total Queries: {metadata.get('total_queries', 0)}\")\n",
        "    print(f\"  Topics: {', '.join(metadata.get('topics', []))}\")\n",
        "else:\n",
        "    print(\"Evaluation pipeline failed or no results available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SPICED Dataset Integration\n",
        "\n",
        "This evaluation uses the **SPICED (Similarity Detection in News)** dataset with proper train/test splits and difficulty-based evaluation:\n",
        "\n",
        "### Dataset Structure\n",
        "- **Combined Dataset**: 977 pairs total (679 train, 298 test)\n",
        "- **Intratopic Pairs**: Same topic, similar content (easier)\n",
        "- **Intertopic Pairs**: Different topics, similar content (harder)  \n",
        "- **Hard Examples**: Challenging pairs for robustness testing\n",
        "\n",
        "### Evaluation Metrics\n",
        "- **Similarity Detection**: How well the system finds similar articles\n",
        "- **Topic Retrieval**: Accuracy of topic-based recommendations\n",
        "- **Diversity Assessment**: Coverage across different topics\n",
        "- **Difficulty-Based Performance**: Performance on different complexity levels\n",
        "\n",
        "### Ground Truth\n",
        "The SPICED dataset provides human-verified similarity pairs as ground truth, ensuring reliable evaluation of the recommendation system's performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. User Profiles for Personalization Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert user profiles data to UserProfile objects\n",
        "user_profiles = []\n",
        "for profile_data in user_profiles_data:\n",
        "    user_profile = UserProfile(\n",
        "        id=profile_data['id'],\n",
        "        preferred_topics=profile_data['preferred_topics'],\n",
        "        reading_history=profile_data['reading_history'],\n",
        "        preferred_sources=profile_data['preferred_sources']\n",
        "    )\n",
        "    user_profiles.append(user_profile)\n",
        "\n",
        "print(f\"ðŸ‘¤ Created {len(user_profiles)} user profiles for personalization testing\")\n",
        "print(\"\\nUser Profiles:\")\n",
        "for profile in user_profiles:\n",
        "    print(f\"  {profile.id}: {', '.join(profile.preferred_topics)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
