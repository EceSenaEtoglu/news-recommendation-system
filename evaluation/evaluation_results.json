{
  "metadata": {
    "evaluation_date": "2025-10-13T09:51:29.409469",
    "total_queries": 68,
    "positive_queries": 38,
    "negative_queries": 30,
    "spiced_pairs": 1049,
    "topics": [
      "politics",
      "culture",
      "economics",
      "crimes",
      "disasters",
      "science",
      "sports"
    ],
    "train_pairs": 2215,
    "test_pairs": 1049,
    "positive_ratio": 0.28026692087702576,
    "config_variants": [
      "basic",
      "enhanced",
      "multi_model",
      "high_recall",
      "high_precision"
    ],
    "system_type": "RecommendationSystem"
  },
  "similarity_detection": {
    "config_results": {
      "basic": {
        "queries": [
          {
            "query_id": "spiced_0",
            "topic": "politics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.1273038387298584,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "spiced_1",
            "topic": "politics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.06842160224914551,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "spiced_2",
            "topic": "culture",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.06202077865600586,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "spiced_3",
            "topic": "economics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.06819725036621094,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "spiced_4",
            "topic": "crimes",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.08448410034179688,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "spiced_5",
            "topic": "disasters",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.07542896270751953,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "spiced_6",
            "topic": "science",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.09151935577392578,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "spiced_7",
            "topic": "sports",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.06024622917175293,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "spiced_8",
            "topic": "economics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.07973575592041016,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "spiced_9",
            "topic": "disasters",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.07852673530578613,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "spiced_10",
            "topic": "politics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.0696113109588623,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "spiced_11",
            "topic": "disasters",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.08188199996948242,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "spiced_12",
            "topic": "crimes",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.06222176551818848,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "spiced_13",
            "topic": "culture",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.09017515182495117,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "spiced_14",
            "topic": "crimes",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.08567214012145996,
            "retrieved_count": 0,
            "config_name": "basic"
          }
        ],
        "topic_performance": {
          "politics": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 3
          },
          "culture": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 2
          },
          "economics": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 2
          },
          "crimes": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 3
          },
          "disasters": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 3
          },
          "science": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 1
          },
          "sports": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 1
          }
        },
        "overall_metrics": {
          "MRR": 0.0,
          "Hit@1": 0.0,
          "Hit@3": 0.0,
          "Hit@10": 0.0,
          "avg_query_time": 0.07902979850769043
        },
        "binary_classification": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "accuracy": 0.0,
          "true_positives": 0,
          "false_positives": 0,
          "true_negatives": 0,
          "false_negatives": 15
        }
      },
      "enhanced": {
        "queries": [
          {
            "query_id": "spiced_0",
            "topic": "politics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.11858773231506348,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "spiced_1",
            "topic": "politics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.15164566040039062,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "spiced_2",
            "topic": "culture",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.1408834457397461,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "spiced_3",
            "topic": "economics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.12820744514465332,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "spiced_4",
            "topic": "crimes",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.11925506591796875,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "spiced_5",
            "topic": "disasters",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.12561655044555664,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "spiced_6",
            "topic": "science",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.13482379913330078,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "spiced_7",
            "topic": "sports",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.10717058181762695,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "spiced_8",
            "topic": "economics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.13777875900268555,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "spiced_9",
            "topic": "disasters",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.13512277603149414,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "spiced_10",
            "topic": "politics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.09311389923095703,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "spiced_11",
            "topic": "disasters",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.13207745552062988,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "spiced_12",
            "topic": "crimes",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.13990044593811035,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "spiced_13",
            "topic": "culture",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.11820292472839355,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "spiced_14",
            "topic": "crimes",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 0.10950613021850586,
            "retrieved_count": 0,
            "config_name": "enhanced"
          }
        ],
        "topic_performance": {
          "politics": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 3
          },
          "culture": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 2
          },
          "economics": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 2
          },
          "crimes": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 3
          },
          "disasters": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 3
          },
          "science": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 1
          },
          "sports": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 1
          }
        },
        "overall_metrics": {
          "MRR": 0.0,
          "Hit@1": 0.0,
          "Hit@3": 0.0,
          "Hit@10": 0.0,
          "avg_query_time": 0.1261261781056722
        },
        "binary_classification": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "accuracy": 0.0,
          "true_positives": 0,
          "false_positives": 0,
          "true_negatives": 0,
          "false_negatives": 15
        }
      },
      "multi_model": {
        "queries": [
          {
            "query_id": "spiced_0",
            "topic": "politics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 6.30063271522522,
            "retrieved_count": 4,
            "config_name": "multi_model"
          },
          {
            "query_id": "spiced_1",
            "topic": "politics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 6.474690675735474,
            "retrieved_count": 4,
            "config_name": "multi_model"
          },
          {
            "query_id": "spiced_2",
            "topic": "culture",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 7.285998821258545,
            "retrieved_count": 4,
            "config_name": "multi_model"
          },
          {
            "query_id": "spiced_3",
            "topic": "economics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 7.019049882888794,
            "retrieved_count": 3,
            "config_name": "multi_model"
          },
          {
            "query_id": "spiced_4",
            "topic": "crimes",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 6.810312747955322,
            "retrieved_count": 4,
            "config_name": "multi_model"
          },
          {
            "query_id": "spiced_5",
            "topic": "disasters",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 7.071383714675903,
            "retrieved_count": 4,
            "config_name": "multi_model"
          },
          {
            "query_id": "spiced_6",
            "topic": "science",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 7.712047576904297,
            "retrieved_count": 4,
            "config_name": "multi_model"
          },
          {
            "query_id": "spiced_7",
            "topic": "sports",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 5.505037784576416,
            "retrieved_count": 4,
            "config_name": "multi_model"
          },
          {
            "query_id": "spiced_8",
            "topic": "economics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 5.959989547729492,
            "retrieved_count": 3,
            "config_name": "multi_model"
          },
          {
            "query_id": "spiced_9",
            "topic": "disasters",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 6.1266560554504395,
            "retrieved_count": 3,
            "config_name": "multi_model"
          },
          {
            "query_id": "spiced_10",
            "topic": "politics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 5.9633948802948,
            "retrieved_count": 3,
            "config_name": "multi_model"
          },
          {
            "query_id": "spiced_11",
            "topic": "disasters",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 5.745035648345947,
            "retrieved_count": 3,
            "config_name": "multi_model"
          },
          {
            "query_id": "spiced_12",
            "topic": "crimes",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 5.654938220977783,
            "retrieved_count": 4,
            "config_name": "multi_model"
          },
          {
            "query_id": "spiced_13",
            "topic": "culture",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 5.562866687774658,
            "retrieved_count": 4,
            "config_name": "multi_model"
          },
          {
            "query_id": "spiced_14",
            "topic": "crimes",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 5.849913835525513,
            "retrieved_count": 4,
            "config_name": "multi_model"
          }
        ],
        "topic_performance": {
          "politics": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 3
          },
          "culture": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 2
          },
          "economics": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 2
          },
          "crimes": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 3
          },
          "disasters": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 3
          },
          "science": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 1
          },
          "sports": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 1
          }
        },
        "overall_metrics": {
          "MRR": 0.0,
          "Hit@1": 0.0,
          "Hit@3": 0.0,
          "Hit@10": 0.0,
          "avg_query_time": 6.336129919687907
        },
        "binary_classification": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "accuracy": 0.0,
          "true_positives": 0,
          "false_positives": 0,
          "true_negatives": 0,
          "false_negatives": 15
        }
      },
      "high_recall": {
        "queries": [
          {
            "query_id": "spiced_0",
            "topic": "politics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 9.882212162017822,
            "retrieved_count": 4,
            "config_name": "high_recall"
          },
          {
            "query_id": "spiced_1",
            "topic": "politics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 9.624997615814209,
            "retrieved_count": 4,
            "config_name": "high_recall"
          },
          {
            "query_id": "spiced_2",
            "topic": "culture",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 9.929807186126709,
            "retrieved_count": 4,
            "config_name": "high_recall"
          },
          {
            "query_id": "spiced_3",
            "topic": "economics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 9.83396029472351,
            "retrieved_count": 3,
            "config_name": "high_recall"
          },
          {
            "query_id": "spiced_4",
            "topic": "crimes",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 9.747591257095337,
            "retrieved_count": 4,
            "config_name": "high_recall"
          },
          {
            "query_id": "spiced_5",
            "topic": "disasters",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 11.182536363601685,
            "retrieved_count": 4,
            "config_name": "high_recall"
          },
          {
            "query_id": "spiced_6",
            "topic": "science",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 10.95788860321045,
            "retrieved_count": 4,
            "config_name": "high_recall"
          },
          {
            "query_id": "spiced_7",
            "topic": "sports",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 9.449925661087036,
            "retrieved_count": 4,
            "config_name": "high_recall"
          },
          {
            "query_id": "spiced_8",
            "topic": "economics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 10.438151597976685,
            "retrieved_count": 3,
            "config_name": "high_recall"
          },
          {
            "query_id": "spiced_9",
            "topic": "disasters",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 11.412929058074951,
            "retrieved_count": 3,
            "config_name": "high_recall"
          },
          {
            "query_id": "spiced_10",
            "topic": "politics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 10.182305574417114,
            "retrieved_count": 3,
            "config_name": "high_recall"
          },
          {
            "query_id": "spiced_11",
            "topic": "disasters",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 10.716527700424194,
            "retrieved_count": 3,
            "config_name": "high_recall"
          },
          {
            "query_id": "spiced_12",
            "topic": "crimes",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 10.369624853134155,
            "retrieved_count": 4,
            "config_name": "high_recall"
          },
          {
            "query_id": "spiced_13",
            "topic": "culture",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 9.408774852752686,
            "retrieved_count": 4,
            "config_name": "high_recall"
          },
          {
            "query_id": "spiced_14",
            "topic": "crimes",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 9.44978666305542,
            "retrieved_count": 4,
            "config_name": "high_recall"
          }
        ],
        "topic_performance": {
          "politics": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 3
          },
          "culture": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 2
          },
          "economics": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 2
          },
          "crimes": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 3
          },
          "disasters": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 3
          },
          "science": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 1
          },
          "sports": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 1
          }
        },
        "overall_metrics": {
          "MRR": 0.0,
          "Hit@1": 0.0,
          "Hit@3": 0.0,
          "Hit@10": 0.0,
          "avg_query_time": 10.172467962900798
        },
        "binary_classification": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "accuracy": 0.0,
          "true_positives": 0,
          "false_positives": 0,
          "true_negatives": 0,
          "false_negatives": 15
        }
      },
      "high_precision": {
        "queries": [
          {
            "query_id": "spiced_0",
            "topic": "politics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 4.010149002075195,
            "retrieved_count": 4,
            "config_name": "high_precision"
          },
          {
            "query_id": "spiced_1",
            "topic": "politics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 3.9996159076690674,
            "retrieved_count": 4,
            "config_name": "high_precision"
          },
          {
            "query_id": "spiced_2",
            "topic": "culture",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 4.110182523727417,
            "retrieved_count": 4,
            "config_name": "high_precision"
          },
          {
            "query_id": "spiced_3",
            "topic": "economics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 4.023140907287598,
            "retrieved_count": 3,
            "config_name": "high_precision"
          },
          {
            "query_id": "spiced_4",
            "topic": "crimes",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 4.337156057357788,
            "retrieved_count": 4,
            "config_name": "high_precision"
          },
          {
            "query_id": "spiced_5",
            "topic": "disasters",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 4.459881067276001,
            "retrieved_count": 4,
            "config_name": "high_precision"
          },
          {
            "query_id": "spiced_6",
            "topic": "science",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 4.469747543334961,
            "retrieved_count": 4,
            "config_name": "high_precision"
          },
          {
            "query_id": "spiced_7",
            "topic": "sports",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 4.087598562240601,
            "retrieved_count": 4,
            "config_name": "high_precision"
          },
          {
            "query_id": "spiced_8",
            "topic": "economics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 4.067559003829956,
            "retrieved_count": 3,
            "config_name": "high_precision"
          },
          {
            "query_id": "spiced_9",
            "topic": "disasters",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 4.532558441162109,
            "retrieved_count": 3,
            "config_name": "high_precision"
          },
          {
            "query_id": "spiced_10",
            "topic": "politics",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 4.8124098777771,
            "retrieved_count": 3,
            "config_name": "high_precision"
          },
          {
            "query_id": "spiced_11",
            "topic": "disasters",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 4.185856580734253,
            "retrieved_count": 3,
            "config_name": "high_precision"
          },
          {
            "query_id": "spiced_12",
            "topic": "crimes",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 4.278966665267944,
            "retrieved_count": 4,
            "config_name": "high_precision"
          },
          {
            "query_id": "spiced_13",
            "topic": "culture",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 4.280141592025757,
            "retrieved_count": 4,
            "config_name": "high_precision"
          },
          {
            "query_id": "spiced_14",
            "topic": "crimes",
            "rank": null,
            "hit@1": 0,
            "hit@3": 0,
            "hit@10": 0,
            "binary_prediction": 0,
            "query_time": 4.4126622676849365,
            "retrieved_count": 4,
            "config_name": "high_precision"
          }
        ],
        "topic_performance": {
          "politics": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 3
          },
          "culture": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 2
          },
          "economics": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 2
          },
          "crimes": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 3
          },
          "disasters": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 3
          },
          "science": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 1
          },
          "sports": {
            "MRR": 0.0,
            "Hit@3": 0.0,
            "n": 1
          }
        },
        "overall_metrics": {
          "MRR": 0.0,
          "Hit@1": 0.0,
          "Hit@3": 0.0,
          "Hit@10": 0.0,
          "avg_query_time": 4.271175066630046
        },
        "binary_classification": {
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "accuracy": 0.0,
          "true_positives": 0,
          "false_positives": 0,
          "true_negatives": 0,
          "false_negatives": 15
        }
      }
    },
    "overall_comparison": {
      "config_rankings": {
        "by_MRR": [
          [
            "basic",
            {
              "MRR": 0.0,
              "Hit@3": 0.0,
              "Hit@1": 0.0,
              "avg_query_time": 0.07902979850769043
            }
          ],
          [
            "enhanced",
            {
              "MRR": 0.0,
              "Hit@3": 0.0,
              "Hit@1": 0.0,
              "avg_query_time": 0.1261261781056722
            }
          ],
          [
            "multi_model",
            {
              "MRR": 0.0,
              "Hit@3": 0.0,
              "Hit@1": 0.0,
              "avg_query_time": 6.336129919687907
            }
          ],
          [
            "high_recall",
            {
              "MRR": 0.0,
              "Hit@3": 0.0,
              "Hit@1": 0.0,
              "avg_query_time": 10.172467962900798
            }
          ],
          [
            "high_precision",
            {
              "MRR": 0.0,
              "Hit@3": 0.0,
              "Hit@1": 0.0,
              "avg_query_time": 4.271175066630046
            }
          ]
        ],
        "by_Hit@3": [
          [
            "basic",
            {
              "MRR": 0.0,
              "Hit@3": 0.0,
              "Hit@1": 0.0,
              "avg_query_time": 0.07902979850769043
            }
          ],
          [
            "enhanced",
            {
              "MRR": 0.0,
              "Hit@3": 0.0,
              "Hit@1": 0.0,
              "avg_query_time": 0.1261261781056722
            }
          ],
          [
            "multi_model",
            {
              "MRR": 0.0,
              "Hit@3": 0.0,
              "Hit@1": 0.0,
              "avg_query_time": 6.336129919687907
            }
          ],
          [
            "high_recall",
            {
              "MRR": 0.0,
              "Hit@3": 0.0,
              "Hit@1": 0.0,
              "avg_query_time": 10.172467962900798
            }
          ],
          [
            "high_precision",
            {
              "MRR": 0.0,
              "Hit@3": 0.0,
              "Hit@1": 0.0,
              "avg_query_time": 4.271175066630046
            }
          ]
        ],
        "by_speed": [
          [
            "basic",
            {
              "MRR": 0.0,
              "Hit@3": 0.0,
              "Hit@1": 0.0,
              "avg_query_time": 0.07902979850769043
            }
          ],
          [
            "enhanced",
            {
              "MRR": 0.0,
              "Hit@3": 0.0,
              "Hit@1": 0.0,
              "avg_query_time": 0.1261261781056722
            }
          ],
          [
            "high_precision",
            {
              "MRR": 0.0,
              "Hit@3": 0.0,
              "Hit@1": 0.0,
              "avg_query_time": 4.271175066630046
            }
          ],
          [
            "multi_model",
            {
              "MRR": 0.0,
              "Hit@3": 0.0,
              "Hit@1": 0.0,
              "avg_query_time": 6.336129919687907
            }
          ],
          [
            "high_recall",
            {
              "MRR": 0.0,
              "Hit@3": 0.0,
              "Hit@1": 0.0,
              "avg_query_time": 10.172467962900798
            }
          ]
        ]
      },
      "best_configs": {
        "best_MRR": "basic",
        "best_Hit@3": "basic",
        "fastest": "basic"
      },
      "performance_summary": {
        "basic": {
          "MRR": 0.0,
          "Hit@3": 0.0,
          "Hit@1": 0.0,
          "avg_query_time": 0.07902979850769043
        },
        "enhanced": {
          "MRR": 0.0,
          "Hit@3": 0.0,
          "Hit@1": 0.0,
          "avg_query_time": 0.1261261781056722
        },
        "multi_model": {
          "MRR": 0.0,
          "Hit@3": 0.0,
          "Hit@1": 0.0,
          "avg_query_time": 6.336129919687907
        },
        "high_recall": {
          "MRR": 0.0,
          "Hit@3": 0.0,
          "Hit@1": 0.0,
          "avg_query_time": 10.172467962900798
        },
        "high_precision": {
          "MRR": 0.0,
          "Hit@3": 0.0,
          "Hit@1": 0.0,
          "avg_query_time": 4.271175066630046
        }
      }
    }
  },
  "negative_detection": {
    "queries": [],
    "pair_type_performance": {},
    "overall_metrics": {},
    "false_positive_analysis": {}
  },
  "topic_retrieval": {
    "config_results": {
      "basic": {
        "topic_queries": [
          {
            "query_id": "topic_crimes",
            "topic": "crimes",
            "topic_relevance": 0.0,
            "query_time": 0.12440776824951172,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "topic_culture",
            "topic": "culture",
            "topic_relevance": 0.0,
            "query_time": 0.03821539878845215,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "topic_disasters",
            "topic": "disasters",
            "topic_relevance": 0.0,
            "query_time": 0.017641544342041016,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "topic_economics",
            "topic": "economics",
            "topic_relevance": 0.0,
            "query_time": 0.04572272300720215,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "topic_politics",
            "topic": "politics",
            "topic_relevance": 0.0,
            "query_time": 0.04876399040222168,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "topic_science",
            "topic": "science",
            "topic_relevance": 0.0,
            "query_time": 0.025432348251342773,
            "retrieved_count": 0,
            "config_name": "basic"
          },
          {
            "query_id": "topic_sports",
            "topic": "sports",
            "topic_relevance": 0.0,
            "query_time": 0.019946813583374023,
            "retrieved_count": 0,
            "config_name": "basic"
          }
        ],
        "topic_metrics": {
          "politics": {
            "mean_relevance": 0.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "culture": {
            "mean_relevance": 0.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "economics": {
            "mean_relevance": 0.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "crimes": {
            "mean_relevance": 0.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "disasters": {
            "mean_relevance": 0.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "science": {
            "mean_relevance": 0.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "sports": {
            "mean_relevance": 0.0,
            "std_relevance": 0.0,
            "query_count": 1
          }
        }
      },
      "enhanced": {
        "topic_queries": [
          {
            "query_id": "topic_crimes",
            "topic": "crimes",
            "topic_relevance": 0.0,
            "query_time": 0.0723879337310791,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "topic_culture",
            "topic": "culture",
            "topic_relevance": 0.0,
            "query_time": 0.05519986152648926,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "topic_disasters",
            "topic": "disasters",
            "topic_relevance": 0.0,
            "query_time": 0.03957056999206543,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "topic_economics",
            "topic": "economics",
            "topic_relevance": 0.0,
            "query_time": 0.06372594833374023,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "topic_politics",
            "topic": "politics",
            "topic_relevance": 0.0,
            "query_time": 0.059764862060546875,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "topic_science",
            "topic": "science",
            "topic_relevance": 0.0,
            "query_time": 0.04991292953491211,
            "retrieved_count": 0,
            "config_name": "enhanced"
          },
          {
            "query_id": "topic_sports",
            "topic": "sports",
            "topic_relevance": 0.0,
            "query_time": 0.04709768295288086,
            "retrieved_count": 0,
            "config_name": "enhanced"
          }
        ],
        "topic_metrics": {
          "politics": {
            "mean_relevance": 0.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "culture": {
            "mean_relevance": 0.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "economics": {
            "mean_relevance": 0.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "crimes": {
            "mean_relevance": 0.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "disasters": {
            "mean_relevance": 0.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "science": {
            "mean_relevance": 0.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "sports": {
            "mean_relevance": 0.0,
            "std_relevance": 0.0,
            "query_count": 1
          }
        }
      },
      "multi_model": {
        "topic_queries": [
          {
            "query_id": "topic_crimes",
            "topic": "crimes",
            "topic_relevance": 1.0,
            "query_time": 3.6903858184814453,
            "retrieved_count": 4,
            "config_name": "multi_model"
          },
          {
            "query_id": "topic_culture",
            "topic": "culture",
            "topic_relevance": 1.0,
            "query_time": 3.6377601623535156,
            "retrieved_count": 5,
            "config_name": "multi_model"
          },
          {
            "query_id": "topic_disasters",
            "topic": "disasters",
            "topic_relevance": 1.0,
            "query_time": 3.7460787296295166,
            "retrieved_count": 4,
            "config_name": "multi_model"
          },
          {
            "query_id": "topic_economics",
            "topic": "economics",
            "topic_relevance": 1.0,
            "query_time": 3.7189366817474365,
            "retrieved_count": 5,
            "config_name": "multi_model"
          },
          {
            "query_id": "topic_politics",
            "topic": "politics",
            "topic_relevance": 1.0,
            "query_time": 3.5802061557769775,
            "retrieved_count": 5,
            "config_name": "multi_model"
          },
          {
            "query_id": "topic_science",
            "topic": "science",
            "topic_relevance": 1.0,
            "query_time": 3.5079779624938965,
            "retrieved_count": 5,
            "config_name": "multi_model"
          },
          {
            "query_id": "topic_sports",
            "topic": "sports",
            "topic_relevance": 1.0,
            "query_time": 3.5273585319519043,
            "retrieved_count": 5,
            "config_name": "multi_model"
          }
        ],
        "topic_metrics": {
          "politics": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "culture": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "economics": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "crimes": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "disasters": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "science": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "sports": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          }
        }
      },
      "high_recall": {
        "topic_queries": [
          {
            "query_id": "topic_crimes",
            "topic": "crimes",
            "topic_relevance": 1.0,
            "query_time": 5.751067161560059,
            "retrieved_count": 5,
            "config_name": "high_recall"
          },
          {
            "query_id": "topic_culture",
            "topic": "culture",
            "topic_relevance": 1.0,
            "query_time": 5.906231164932251,
            "retrieved_count": 5,
            "config_name": "high_recall"
          },
          {
            "query_id": "topic_disasters",
            "topic": "disasters",
            "topic_relevance": 1.0,
            "query_time": 6.37185263633728,
            "retrieved_count": 4,
            "config_name": "high_recall"
          },
          {
            "query_id": "topic_economics",
            "topic": "economics",
            "topic_relevance": 1.0,
            "query_time": 5.783087730407715,
            "retrieved_count": 5,
            "config_name": "high_recall"
          },
          {
            "query_id": "topic_politics",
            "topic": "politics",
            "topic_relevance": 1.0,
            "query_time": 6.0746238231658936,
            "retrieved_count": 5,
            "config_name": "high_recall"
          },
          {
            "query_id": "topic_science",
            "topic": "science",
            "topic_relevance": 1.0,
            "query_time": 6.330326795578003,
            "retrieved_count": 5,
            "config_name": "high_recall"
          },
          {
            "query_id": "topic_sports",
            "topic": "sports",
            "topic_relevance": 1.0,
            "query_time": 17.55341911315918,
            "retrieved_count": 5,
            "config_name": "high_recall"
          }
        ],
        "topic_metrics": {
          "politics": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "culture": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "economics": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "crimes": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "disasters": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "science": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "sports": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          }
        }
      },
      "high_precision": {
        "topic_queries": [
          {
            "query_id": "topic_crimes",
            "topic": "crimes",
            "topic_relevance": 1.0,
            "query_time": 31.22245717048645,
            "retrieved_count": 4,
            "config_name": "high_precision"
          },
          {
            "query_id": "topic_culture",
            "topic": "culture",
            "topic_relevance": 1.0,
            "query_time": 12.815810203552246,
            "retrieved_count": 5,
            "config_name": "high_precision"
          },
          {
            "query_id": "topic_disasters",
            "topic": "disasters",
            "topic_relevance": 1.0,
            "query_time": 4.6556925773620605,
            "retrieved_count": 4,
            "config_name": "high_precision"
          },
          {
            "query_id": "topic_economics",
            "topic": "economics",
            "topic_relevance": 1.0,
            "query_time": 2.4103317260742188,
            "retrieved_count": 5,
            "config_name": "high_precision"
          },
          {
            "query_id": "topic_politics",
            "topic": "politics",
            "topic_relevance": 1.0,
            "query_time": 2.478253126144409,
            "retrieved_count": 5,
            "config_name": "high_precision"
          },
          {
            "query_id": "topic_science",
            "topic": "science",
            "topic_relevance": 1.0,
            "query_time": 2.459752321243286,
            "retrieved_count": 5,
            "config_name": "high_precision"
          },
          {
            "query_id": "topic_sports",
            "topic": "sports",
            "topic_relevance": 1.0,
            "query_time": 2.3800978660583496,
            "retrieved_count": 5,
            "config_name": "high_precision"
          }
        ],
        "topic_metrics": {
          "politics": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "culture": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "economics": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "crimes": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "disasters": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "science": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          },
          "sports": {
            "mean_relevance": 1.0,
            "std_relevance": 0.0,
            "query_count": 1
          }
        }
      }
    },
    "overall_comparison": {
      "config_rankings": {
        "by_topic_relevance": [
          [
            "multi_model",
            {
              "mean_topic_relevance": 1.0,
              "std_topic_relevance": 0.0,
              "topics_covered": 7
            }
          ],
          [
            "high_recall",
            {
              "mean_topic_relevance": 1.0,
              "std_topic_relevance": 0.0,
              "topics_covered": 7
            }
          ],
          [
            "high_precision",
            {
              "mean_topic_relevance": 1.0,
              "std_topic_relevance": 0.0,
              "topics_covered": 7
            }
          ],
          [
            "basic",
            {
              "mean_topic_relevance": 0.0,
              "std_topic_relevance": 0.0,
              "topics_covered": 7
            }
          ],
          [
            "enhanced",
            {
              "mean_topic_relevance": 0.0,
              "std_topic_relevance": 0.0,
              "topics_covered": 7
            }
          ]
        ]
      },
      "best_configs": {
        "best_topic_relevance": "multi_model"
      },
      "performance_summary": {
        "basic": {
          "mean_topic_relevance": 0.0,
          "std_topic_relevance": 0.0,
          "topics_covered": 7
        },
        "enhanced": {
          "mean_topic_relevance": 0.0,
          "std_topic_relevance": 0.0,
          "topics_covered": 7
        },
        "multi_model": {
          "mean_topic_relevance": 1.0,
          "std_topic_relevance": 0.0,
          "topics_covered": 7
        },
        "high_recall": {
          "mean_topic_relevance": 1.0,
          "std_topic_relevance": 0.0,
          "topics_covered": 7
        },
        "high_precision": {
          "mean_topic_relevance": 1.0,
          "std_topic_relevance": 0.0,
          "topics_covered": 7
        }
      }
    }
  },
  "diversity": {
    "config_results": {
      "basic": {
        "diversity_queries": [
          {
            "query_id": "diversity_cross_topic",
            "topic_diversity": 0.0,
            "content_diversity": 0.0,
            "overall_diversity": 0.0,
            "config_name": "basic"
          }
        ],
        "diversity_metrics": {
          "mean_diversity": 0.0,
          "std_diversity": 0.0,
          "min_diversity": 0.0,
          "max_diversity": 0.0
        }
      },
      "enhanced": {
        "diversity_queries": [
          {
            "query_id": "diversity_cross_topic",
            "topic_diversity": 0.0,
            "content_diversity": 0.0,
            "overall_diversity": 0.0,
            "config_name": "enhanced"
          }
        ],
        "diversity_metrics": {
          "mean_diversity": 0.0,
          "std_diversity": 0.0,
          "min_diversity": 0.0,
          "max_diversity": 0.0
        }
      },
      "multi_model": {
        "diversity_queries": [
          {
            "query_id": "diversity_cross_topic",
            "topic_diversity": 1.0,
            "content_diversity": 0.437,
            "overall_diversity": 0.7185,
            "config_name": "multi_model"
          }
        ],
        "diversity_metrics": {
          "mean_diversity": 0.7185,
          "std_diversity": 0.0,
          "min_diversity": 0.7185,
          "max_diversity": 0.7185
        }
      },
      "high_recall": {
        "diversity_queries": [
          {
            "query_id": "diversity_cross_topic",
            "topic_diversity": 1.0,
            "content_diversity": 0.424,
            "overall_diversity": 0.712,
            "config_name": "high_recall"
          }
        ],
        "diversity_metrics": {
          "mean_diversity": 0.712,
          "std_diversity": 0.0,
          "min_diversity": 0.712,
          "max_diversity": 0.712
        }
      },
      "high_precision": {
        "diversity_queries": [
          {
            "query_id": "diversity_cross_topic",
            "topic_diversity": 0.8571428571428571,
            "content_diversity": 0.403,
            "overall_diversity": 0.6300714285714286,
            "config_name": "high_precision"
          }
        ],
        "diversity_metrics": {
          "mean_diversity": 0.6300714285714286,
          "std_diversity": 0.0,
          "min_diversity": 0.6300714285714286,
          "max_diversity": 0.6300714285714286
        }
      }
    },
    "overall_comparison": {
      "config_rankings": {
        "by_diversity": [
          [
            "multi_model",
            {
              "mean_diversity": 0.7185,
              "std_diversity": 0.0,
              "min_diversity": 0.7185,
              "max_diversity": 0.7185
            }
          ],
          [
            "high_recall",
            {
              "mean_diversity": 0.712,
              "std_diversity": 0.0,
              "min_diversity": 0.712,
              "max_diversity": 0.712
            }
          ],
          [
            "high_precision",
            {
              "mean_diversity": 0.6300714285714286,
              "std_diversity": 0.0,
              "min_diversity": 0.6300714285714286,
              "max_diversity": 0.6300714285714286
            }
          ],
          [
            "basic",
            {
              "mean_diversity": 0.0,
              "std_diversity": 0.0,
              "min_diversity": 0.0,
              "max_diversity": 0.0
            }
          ],
          [
            "enhanced",
            {
              "mean_diversity": 0.0,
              "std_diversity": 0.0,
              "min_diversity": 0.0,
              "max_diversity": 0.0
            }
          ]
        ]
      },
      "best_configs": {
        "best_diversity": "multi_model"
      },
      "performance_summary": {
        "basic": {
          "mean_diversity": 0.0,
          "std_diversity": 0.0,
          "min_diversity": 0.0,
          "max_diversity": 0.0
        },
        "enhanced": {
          "mean_diversity": 0.0,
          "std_diversity": 0.0,
          "min_diversity": 0.0,
          "max_diversity": 0.0
        },
        "multi_model": {
          "mean_diversity": 0.7185,
          "std_diversity": 0.0,
          "min_diversity": 0.7185,
          "max_diversity": 0.7185
        },
        "high_recall": {
          "mean_diversity": 0.712,
          "std_diversity": 0.0,
          "min_diversity": 0.712,
          "max_diversity": 0.712
        },
        "high_precision": {
          "mean_diversity": 0.6300714285714286,
          "std_diversity": 0.0,
          "min_diversity": 0.6300714285714286,
          "max_diversity": 0.6300714285714286
        }
      }
    }
  },
  "difficulty_levels": {
    "intratopic_negatives": {
      "avg_score": 0.0,
      "total_pairs": 0,
      "sampled_pairs": 6,
      "difficulty_type": "intratopic",
      "topic_metrics": {}
    },
    "intertopic_negatives": {
      "avg_score": 0.0,
      "total_pairs": 0,
      "sampled_pairs": 6,
      "difficulty_type": "intertopic",
      "topic_metrics": {}
    },
    "hard_negatives": {
      "avg_score": 0.0,
      "total_pairs": 0,
      "sampled_pairs": 6,
      "difficulty_type": "hard",
      "topic_metrics": {}
    },
    "difficulty_comparison": {
      "scores": {
        "intratopic_negatives": 0.0,
        "intertopic_negatives": 0.0,
        "hard_negatives": 0.0
      },
      "ranking": [
        [
          "intratopic_negatives",
          0.0
        ],
        [
          "intertopic_negatives",
          0.0
        ],
        [
          "hard_negatives",
          0.0
        ]
      ],
      "hardest_level": "intratopic_negatives"
    }
  },
  "baseline_comparison": {
    "bm25_baseline": {
      "method": "BM25 (per-URL ranking)",
      "MRR": 0.5,
      "Hit@3": 1.0,
      "Hit@10": 1.0,
      "total_queries": 5,
      "avg_rank": 1.0,
      "total_documents": 570
    },
    "random_baseline": {
      "method": "Random",
      "MRR": 0.005969865404213683,
      "Hit@3": 0.0,
      "Hit@10": 0.0,
      "total_queries": 10,
      "avg_rank": 340.1
    },
    "tfidf_baseline": {
      "method": "TF-IDF (pairwise similarity)",
      "avg_similarity": 0.667464623632609,
      "total_queries": 5,
      "min_similarity": 0.25021438837536775,
      "max_similarity": 0.8842542545514959,
      "std_similarity": 0.21884837958947628
    }
  },
  "config_analysis": {
    "config_descriptions": {
      "basic": "Minimal features - no graph RAG, no cross-encoder, small K values",
      "enhanced": "Graph RAG enabled - entity boosting, medium K values",
      "multi_model": "Full pipeline - graph RAG + cross-encoder, balanced K values",
      "high_recall": "High recall - large K values, more candidates",
      "high_precision": "High precision - focused search, high topic bonuses"
    },
    "performance_insights": {
      "expected_tradeoffs": {
        "basic": "Fastest but lowest quality",
        "enhanced": "Good balance of speed and quality",
        "multi_model": "Best quality but slower",
        "high_recall": "Finds more relevant items but may include noise",
        "high_precision": "High quality results but may miss some relevant items"
      },
      "use_cases": {
        "basic": "Real-time applications, mobile devices",
        "enhanced": "General purpose recommendations",
        "multi_model": "High-quality recommendations, desktop/web",
        "high_recall": "Research, comprehensive analysis",
        "high_precision": "Curated content, premium features"
      }
    },
    "recommendations": {}
  },
  "overall_performance": {
    "similarity_score": 0.0,
    "topic_relevance": 0.6,
    "diversity_score": 0.4121142857142857,
    "intratopic_score": 0.0,
    "intertopic_score": 0.0,
    "hard_examples_score": 0.0,
    "overall_score": 0.10373714285714286,
    "config_performance": {},
    "negative_detection_score": 1.0
  }
}